services:
  llama:
    build: .
    container_name: llama
    volumes:
      - "./models:/models" 
      - "./config.yml:/app/config.yml"
    ports:
      - "8080:8080"
    
    # --- A PARTE MAIS IMPORTANTE PARA CUDA ---
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all # Isso ativa as suas DUAS placas automagicamente
              capabilities: [gpu]
    
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    
    # Configuração de execução
    command: >
      --model_name /models/Dolphin3.0-Qwen2.5-0.5B-exl2
      --host 0.0.0.0
      --port 8080
      --n-gpu-layers 99
      --ctx-size 32000