services:
  llama:
    build: .
    container_name: llama
    volumes:
      - "./models:/models"
      # O volume do vault continua aqui para os hooks se necessário
      - "/home/jeo/Documents/ObsidianVault:/vault"
    ports:
      - "8080:8080"
    
    # --- A PARTE MAIS IMPORTANTE PARA CUDA ---
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all # Isso ativa as suas DUAS placas automagicamente
              capabilities: [gpu]
    
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    
    # Configuração de execução para tirar proveito das 2 GPUs
    command: >
      --model /models/Llama-3.2-3B-Instruct-Q8_0.gguf
      --host 0.0.0.0
      --port 8080
      --n-gpu-layers 99   # Garante que o modelo vá todo para a VRAM
      --ctx-size 50000