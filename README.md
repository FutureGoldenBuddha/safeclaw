# Safeclaw: security tips for openclaw! ğŸ¦ ğŸ«§

### Safeclaw â€” security best practices for OpenClaw ğŸ¦âœ¨

![Banner](./.devcontainer/assets/banner_1280x320.png)

Build a safer, more private OpenClaw setup with **rootless Docker**, a controlled outbound proxy, and sensible defaults so your personal AI behaves like a well-trained lobster â€” useful, contained, and not wandering off into the wild web. ğŸš€ğŸ”’

---

### Why Safeclaw

**What you get**
- **Privacy first** â€” run Llama-compatible models locally and keep **sensitive data** off third-party clouds. ğŸ§ ğŸ”  
<br>
- **Defense in depth** â€” Docker rootless plus a forward proxy (Squid) to control outbound traffic. ğŸ›¡ï¸ğŸ“¡  
<br>
- **Portable and practical** â€” works on any OS that supports rootless Docker and VS Code DevContainers. ğŸ’»ğŸŒ  
<br>
- **Developer-friendly** â€” DevContainer scripts, onboarding helpers, and maintenance toggles so you spend less time configuring and more time asking clever questions. âš™ï¸âœ¨  
<br>
- **GPU-ready** â€” prepared for NVIDIA and Intel GPUs, with Ubuntu 24.04 instructions included. âš¡ğŸ–¥ï¸
<br>

**Tone check**: this repo is for people who want a local assistant without accidentally turning it into a data-leaking gremlin. ğŸ¦–â¡ï¸ğŸ¦

<details>
<summary>**Possible constraints** GPU card(s) and local LLMs</summary>


I have a NVIDIA 4060 RTX with 8GB RAM, so if you have a different GPU and/or use a different model please adjust the context size and maximum tokens accordingly. I can make a list with different models configurations to make it easier, but I am currently using ministral 3B and happy with the results ğŸ˜Š.

A trick to make your agent smarter is to write meaningful skills with bigger models on the web, asking them to make a SKIlL.md, MEMORY_snippet.md (a small section that you add in your main MEMORY.md file) and a README.md (for future reference). Then you place the skill folder with the files in the skills folder (try to say this fast twice in a row ğŸ¥´) and proceed to ask your local AI to "load the skill <skill folder name> and confirm it's loaded". This way your local ai agent gets smarter without overloading its memory files. I think we are going to need skills focusing in security measures also...

</details>

---

### Quick Start

#### Prerequisites
**Install and configure** GPU drivers in your host, and Docker with rootless mode enabled, Visual Studio Code with Dev Containers extension.  ğŸ“¦ğŸ”§

#### Clone the repo

```bash
git clone https://github.com/FutureGoldenBuddha/safeclaw.git
cd safeclaw
```

Place local Llama-compatible model files in `.devcontainer/models/` for best privacy.

Ensure your local model name matches in the root `docker-compose.yml` and `.devcontainer/containers/openclaw/debian/openclaw.json`. ğŸ—‚ï¸âœ…

#### Start the environment

**Option A â€” VS Code DevContainer (recommended)**  
Open the project in VS Code and choose Reopen in Container or run Dev Containers: Reopen in Container from the command palette. ğŸ§©

**Option B â€” Docker Compose (CLI)**

```bash
docker compose up -d
```

Continue from inside the container shell. ğŸš

```bash
docker exec -it openclaw sh
```

#### Maintenance mode for temporary internet access

These containers start with strict network restrictions. To temporarily allow internet access via a controlled Squid proxy (or else you cannot install openclaw), run from the host and inside the `.devcontainer` folder:

```bash
bash maintenance-on.sh
```

When finished installing openclaw, lock it back down:

```bash
bash maintenance-off.sh
```

Think of maintenance-on as opening the hatch for a quick supply run and maintenance-off as closing it again so nothing sneaks out. ğŸšªğŸ”

#### Run post-start setup (if you don't see de openclaw_install folder and its contents)

Run the OpenClaw post-start script from inside the container:

```bash
bash .devcontainer/containers/openclaw/debian/post-start.sh
```

Wait for it to finish. â³

#### Configure OpenClaw

From the `openclaw_install` directory inside the container:

```bash
pnpm openclaw onboard
pnpm openclaw models set llama/Ministral-3-3B-Instruct-2512-Q8_0
pnpm openclaw models list
pnpm openclaw gateway --bind loopback --port 18789 --allow-unconfigured --token gateway-token
```

**Note**: final config is at `openclaw_data/.openclaw/openclaw.json`. If `models.providers` is missing after onboarding, add it manually. ğŸ“ğŸ”

---

### Security Operational Tips

- **Keep Docker rootless** to reduce blast radius. ğŸ§¯  
- **Use local models** for sensitive data and avoid sending PII to external providers. ğŸ—ï¸  
- **Enable the proxy only when needed** and log traffic for audits. ğŸ“‹ğŸ”  
- **Secrets** must never be committed to the repo. Use environment variables or a secrets manager. ğŸ”‘  
- **Patch often** and update OS, drivers, and container images regularly. ğŸ”„  
- **User groups** add your user to `render` and `video` for GPU access. ğŸ§‘â€ğŸ’»

---

### GPU Setup on Ubuntu 24.04

Check your GPU:

```bash
lspci | grep -E 'VGA|3D|Display'
```

#### Common preparation

```bash
sudo apt update && sudo apt upgrade -y
sudo apt install build-essential
lspci -k | grep -EA3 'VGA|3D|Display'
```

#### Intel GPU

```bash
sudo apt install intel-media-va-driver-non-free vainfo
vainfo
sudo apt install intel-opencl-icd clinfo
clinfo | grep "Device Name"
```

If the `-non-free` package is unavailable, use `intel-media-va-driver`. ğŸ§©

#### NVIDIA GPU

```bash
ubuntu-drivers devices
sudo ubuntu-drivers autoinstall
# or
sudo apt install nvidia-driver-550
sudo reboot
nvidia-smi
sudo apt install nvidia-cuda-toolkit
nvcc --version
sudo apt install libva-utils vdpauinfo
vdpauinfo
```

Add your user to GPU groups:

```bash
sudo usermod -a -G render,video $USER
```

Log out and back in for group changes to take effect. ğŸ”

---

### Contribute and Support

- **Test** with local models and open issues for bugs or UX pain points. ğŸ§ª  
- **Star** the repo if Safeclaw saved you from a late-night data leak panic. â­  
- **PRs welcome** for better automation, extra GPU support, or clearer onboarding. ğŸ¤

Open an issue on GitHub for help and weâ€™ll get back to you faster than a lobster can pinch. ğŸ¦âš¡

---

**Ready to be useful?** Clone the repo, follow the Quick Start, and enjoy a local OpenClaw thatâ€™s powerful, private, and delightfully well-behaved. ğŸ¯ğŸ¦